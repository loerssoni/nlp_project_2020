{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 10: Emotion Analysis in Dataset\n",
    "This project aims to investigate the emotion and sentiment from a set of publicly open dataset and test various commonalities for identifying of the emotion. First, collect the emotion dataset from Kaggle available at https://www.kaggle.com/praveengovi/emotions-dataset-for-nlp. Note that there is also a provided for machine learning based approach for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, Table\n",
    "# from sqlalchemy.schema import MetaData\n",
    "engine = create_engine('sqlite:///./project.db', echo=False)\n",
    "!python concat_files.py\n",
    "data = pd.read_csv('./data/full_data.txt', header=None, names=['text','label'], sep=';')\n",
    "harv_inquirer = pd.read_excel('http://www.wjh.harvard.edu/~inquirer/inquirerbasic.xls')\n",
    "harv_inquirer['Entry'] = harv_inquirer.Entry.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength = 'cd C:/Users/lauri/downloads/SentiStrength2.3Free.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tUse the Harvard General Inquirer available in http://www.wjh.harvard.edu/~inquirer/inquirerbasic.xls and try to identify wording associated to each of the five categories “sadness”, “anger”, “love”, “surprise”, “joy”. Record the obtained wording in a separate database D that will be part of deliverables. \n",
    "\n",
    "Note: Description of the harvard columns is here: http://www.wjh.harvard.edu/~inquirer/homecat.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Harvard General Inquirer does not have a nice way to match the categories.\n",
    "These can be toyed around with to filter with different categories to include / exclude for each label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = {\n",
    "    'surprise':{\n",
    "        'include': ['Arousal'],\n",
    "        'exclude': []\n",
    "        },\n",
    "    'joy':{\n",
    "        'include': ['Positiv'],\n",
    "        'exclude': ['Affil']\n",
    "        },\n",
    "    'love':{\n",
    "        'include': ['Affil'],\n",
    "        'exclude': ['Negativ']\n",
    "        },\n",
    "    'anger':{\n",
    "        'include': ['Hostile'],\n",
    "        'exclude': []  \n",
    "        },\n",
    "    'sadness':{\n",
    "        'include': ['Negativ'],\n",
    "        'exclude': ['Hostile']\n",
    "        },\n",
    "    'fear':{\n",
    "        'include': ['Weak'],\n",
    "        'exclude': []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "match_targets = get_all_cat(category_dict, harv_inquirer)\n",
    "match_targets.to_sql('harvardWords', con=engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tUse a simple string matching procedure to evaluate the matching of every utterance to each of the category. The category that yields the highest matching will be assumed to assigned to the underlined Calculate the accuracy of this prediction using the ground truth knowledge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_matches(data, targets):\n",
    "    match_target = pd.get_dummies(targets, columns=['label'], prefix='', prefix_sep='')\n",
    "    match_target = match_target.groupby('entry').sum().reset_index()    \n",
    "    matches = match_target.apply(lambda x: data.text.str.contains(x.entry).astype(int), 1)\n",
    "    matches = match_target.iloc[:,1:].apply(lambda x: matches.mul(x, axis=0).sum(), 0)\n",
    "    return matches.idxmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of string matching:  0.34\n"
     ]
    }
   ],
   "source": [
    "matches = evaluate_matches(data, match_targets)\n",
    "print('Accuracy of string matching: ', (matches == data.label).mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tConsider categories generated by Empath Client https://github.com/Ejhfast/empath-client. Apply Empath Client to each utterance and record categories who held non-zero weights in the database D. Elaborate how you can match these categories to each of the five categories above using appropriate linguistic constructs (entailment, synonymy, hyponymy, hypernymy, etc..). Calculate the accuracy of this prediction approach.\n",
    "\n",
    "Note: pip install empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = process_lexicon(data.text)\n",
    "categories.to_sql('empathCategories', con=engine, index=True, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labels(data, categories, how='min_depth'):\n",
    "    if how == 'exact':\n",
    "        label_pred = categories[categories.isin(data.label)]\n",
    "        label_pred = label_pred.reset_index().groupby('index').last().iloc[:, 0]\n",
    "        label_pred.name = 'label_pred'\n",
    "        com = data.join(label_pred)\n",
    "        return com.label_pred\n",
    "    sims = get_category_similarities(data.label, categories, how)\n",
    "    cat_labels = pd.merge(pd.DataFrame(categories, columns=['category']).reset_index(), sims, on='category',\n",
    "                         how='inner')\n",
    "    com = cat_labels.groupby('index')['label'].agg(lambda x:x.value_counts().index[0])\n",
    "    com = data.join(com, rsuffix='_pred')\n",
    "    return com.label_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Empath:  0.17\n",
      "Accuracy of Empath using exact categories:  0.18\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Empath: ', (evaluate_labels(data, categories) == data.label).mean().round(2))\n",
    "print('Accuracy of Empath using exact categories: ',\n",
    "      (evaluate_labels(data, categories, 'exact') == data.label).mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWe consider the semantic similarity between each of the five categories with every utterance. Use a semantic similarity so that the overall semantic similarity between category C and Utterance S is equal to the arithmetic average of the sum of the Wu and Palmer semantic similarity of C with each noun contained in S (should use part of speech tagger to identify noun category). Report this information in database D. Therefore, for each, utterance, the category that yields the smallest semantic similarity will be assigned to it. Calculate the overall accuracy accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "wup_sims = wup_similarities(data)\n",
    "wup_sims.to_sql('wupSimilarities', con=engine, index=True, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Wu and Palmer similarities:  0.29\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Wu and Palmer similarities: ', (wup_sims.idxmax(1) == data.label).mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tUse the SentiStrength from http://sentistrength.wlv.ac.uk/ to determine the positive, negative and overall (sum of positive and negative) sentiment score for each utterance. Provide this information in database D. Comment on whether the sentiment score can be used an indicator to discriminate the various emotion states.\n",
    "\n",
    "Note: pip install sentistrength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text.to_csv('./data/text.txt', header='text', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process via sentistrength executable manually here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pd.read_csv('./data/text+results.txt', sep='\\t')[['Positive','Negative']]\n",
    "sentiment['Overall'] = sentiment.sum(1)\n",
    "sentiment.to_sql('sentistrength', con=engine, index=True, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tNow we want to develop a machine learning approach for learning to predict the emotion state. For this purpose, tokenize the original data and split the original data into 70% training and 30% testing, and suggest various filtering strategies (e-g-, no filtering, standard stopword removal, selected set of stopwords, …). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tUse various feature engineering, which includes CountVectorizer, tf-Idf for size of vocabulary (all vocabulary,3000, 2000, 1000, 500, 100 of most frequent words). Compare a set of state-of art machine learning classifiers (Naives Bayes, Linear regression, SVM, Decision, Tree and Random Forest). Draw a plot showing the accuracy of the different classifiers and various features. For the classifier that yields the best accuracy, record the confusion matrix, Precision and recall. Compare this to Naives Bayes and Linear regression. Repeat the above reasoning for various filtering strategies to ensure the select the strategy that maximizes the overall accuracy. Provide the result in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWe want to test the performance of deep learning classifier. For this purpose, we shall imitate the  paper available at \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 by Yon. (a Python implementation of the above paper is also available online). Imitate the above reasoning and represent the embedding of each word in sentence using word2vec representation. The features are now represented by the embedding vectors handled in the same way as Yon’s paper above. You should attempt to fine-tune the parameters of the CNN architecture to yield maximum accuracy. Represent the accuracy, precision, recall and confusion matrix of the CNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tRepeat the process of 8) when using FastText embedding instead of word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tDesign a simple GUI of your choice that show the execution of each of the above tasks in a way to ease the task of the assessor or external end-user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
